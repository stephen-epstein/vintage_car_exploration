{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaT Data Farming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get List of Every Result and Save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/stephenepstein/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/stephenepstein/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 678, in reducer_override\n    if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(obj):  # noqa  # pragma: no branch\nRecursionError: maximum recursion depth exceeded in comparison\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/stephenepstein/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/Users/stephenepstein/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/Users/stephenepstein/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/Users/stephenepstein/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 609, in dump\n    raise pickle.PicklingError(msg) from e\n_pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     16\u001b[0m     listings \u001b[39m=\u001b[39m helpers\u001b[39m.\u001b[39mget_listings_no_model(make)\n\u001b[0;32m---> 17\u001b[0m     souplist \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mParallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m)(joblib\u001b[39m.\u001b[39;49mdelayed(get_data)(url) \u001b[39mfor\u001b[39;49;00m url \u001b[39min\u001b[39;49;00m listings)\n\u001b[1;32m     19\u001b[0m     temp \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mjoin(souplist)\n\u001b[1;32m     20\u001b[0m     data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m delimiter \u001b[39m+\u001b[39m temp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1061\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1062\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 938\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    939\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_steph/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_steph/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_steph/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "# Get List of Every Result and Save\n",
    "make = input('Enter Car Make: ')\n",
    "model = input('Enter Car Model: ')\n",
    "delimiter = \"%newline%\"\n",
    "data = ''\n",
    "\n",
    "# Pull HTML data from each url, defined here so that it can run with joblib.Parallel\n",
    "def get_data(url):\n",
    "        r = requests.get(url, allow_redirects=False)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        return str(soup)\n",
    "\n",
    "open(\"./data/HTML/\"+ make + model +\".html\", 'w').close()\n",
    "\n",
    "if model == \"\":\n",
    "    listings = helpers.get_listings_no_model(make)\n",
    "    souplist = joblib.Parallel(n_jobs=-2)(joblib.delayed(get_data)(url) for url in listings)\n",
    "\n",
    "    temp = delimiter.join(souplist)\n",
    "    data += delimiter + temp\n",
    "else:\n",
    "    ids, urls = helpers.get_listings(make, model)\n",
    "    listings = []\n",
    "\n",
    "    # Iterate through BaT specific Id's and show more pages for each listing\n",
    "    urlend = '&results=items'\n",
    "    pgend = 15\n",
    "    for i in range(0,len(ids)):\n",
    "        urlstart = 'https://bringatrailer.com/wp-json/bringatrailer/1.0/data/keyword-filter?bat_keyword_pages=' + ids[i] + '&sort=td&page='\n",
    "        tempurl = helpers.get_urls(urlstart, urlend, pgend, urls)\n",
    "        listings = tempurl\n",
    "        listings = listings[len(ids):len(listings)]\n",
    "        souplist = joblib.Parallel(n_jobs=-2)(joblib.delayed(get_data)(url) for url in listings)\n",
    "\n",
    "        temp = delimiter.join(souplist)\n",
    "        data += delimiter + temp \n",
    "\n",
    "# Write data to file \n",
    "with open(\"./data/HTML/\"+ make + model +\".html\", \"a\") as file:\n",
    "    file.write(data)\n",
    "\n",
    "display('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull Information Off Each Listing Stored in URLS List:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carlist = []\n",
    "Car = []\n",
    "### Read Data and Save to DataFrame ###\n",
    "\n",
    "with open(\"./data/HTML/\"+ make + model +\".html\", \"r\") as file:\n",
    "    test_data = file.read()\n",
    "\n",
    "out = test_data.split(delimiter)\n",
    "\n",
    "#Iterate through each listings and test again to filter out non-car listings using title this time\n",
    "for i in range(0,len(listings)):\n",
    "    soup = BeautifulSoup(out[i], 'html.parser')\n",
    "    title = str(soup.find('title'))\n",
    "    if (\n",
    "        \"Parts\" not in title \n",
    "        and \"Tool\" not in title \n",
    "        and \"Memorabilia\" not in title \n",
    "        and \"Luggage\" not in title \n",
    "        and \"Tool Kit\" not in title \n",
    "        and \"Removable\" not in title \n",
    "        and \"Gearbox\" not in title \n",
    "        and title != 'None' \n",
    "        and 'listing' in listings[i]\n",
    "    ):\n",
    "        loc = str(soup.findAll('a'))\n",
    "        transmission = str(soup.findAll('li'))\n",
    "        contents = str(soup.findAll('p'))\n",
    "        essentials = soup.find('div', class_='essentials')\n",
    "\n",
    "        #Get Car Description\n",
    "        desc = helpers.get_desc(listings[i])\n",
    "\n",
    "        #Get Year\n",
    "        year = desc[0:4]\n",
    "        \n",
    "        #Check if Sold and get highest bid\n",
    "        sold, price = helpers.get_sale_price(title, loc)\n",
    "\n",
    "        #Get Transmission\n",
    "        trans = 'Manual'\n",
    "        if 'Automatic' in transmission:\n",
    "            trans = 'Automatic'\n",
    "\n",
    "        #Get Location\n",
    "        town, state = helpers.get_location(loc)\n",
    "        \n",
    "        #Check for Project\n",
    "        if 'project' in title or 'Project' in title:\n",
    "            project = 1\n",
    "        else:\n",
    "            project = 0\n",
    "        \n",
    "        #Get Lot Number\n",
    "        try:\n",
    "            index = title.index('Lot #')\n",
    "            lotnumb = title[index+5:index+12].partition(')')[0]\n",
    "        except:\n",
    "            lotnumb = \"N/A\"\n",
    "\n",
    "        #Check for Replica\n",
    "        if \"replica\" in title or \"Replica\" in title:\n",
    "            replica = 1\n",
    "        else:\n",
    "            replica = 0\n",
    "\n",
    "        #Get Month\n",
    "        month = helpers.get_month(title)\n",
    "        \n",
    "        #Get Year\n",
    "        if '(' in title:\n",
    "            index = title.index('(')\n",
    "            yearsold = title[index-5:index-1]\n",
    "        else:\n",
    "            index = title.index('20')\n",
    "            yearsold = title[index:index+5]\n",
    "        \n",
    "        #Get Engine and Number Matching\n",
    "        numbmatch = 0\n",
    "        if 'Numbers-Matching' in transmission:\n",
    "            numbmatch = 1\n",
    "        engine = helpers.get_engine(transmission)\n",
    "        \n",
    "        #Get Various Indicators for rust, refurbishment, restoration, scratch, paint bubbles, metal repair, hardtop, or overdrive options\n",
    "        rust, refurbished, restored, scratch, paintbub, metalrepair, hardtop, overdrive, turbo, super = helpers.get_indicators(contents)\n",
    "            \n",
    "        #Get Mileage\n",
    "        miles, milestmu = helpers.get_mileage(transmission)\n",
    "\n",
    "        #Get Descriptions of options and chassis #\n",
    "        chassis, specialdesc, mileagedesc, enginedesc, transdesc, paintdesc, interiordesc, carbdesc, wheeldesc, brakedesc, suspdesc, engine = helpers.get_engine_desc(essentials, engine)\n",
    "\n",
    "        #Check Trans with Description\n",
    "        if 'Manual' in transdesc:\n",
    "            trans = 'Manual'\n",
    "        elif 'Automatic' in transdesc:\n",
    "            trans = 'Automatic'\n",
    "\n",
    "        #Check Mileage with Description (TODO: remove try catch)\n",
    "        try:\n",
    "            if \"K\" in str(miles):\n",
    "                index = mileagedesc.index(\"ilometers\")\n",
    "                tempmile = mileagedesc[0:index].partition(\" \")[0]\n",
    "                if \"k\" in tempmile or \"K\" in tempmile:\n",
    "                    milestmu = re.sub(r\"[^0-9]\", \"\", tempmile)\n",
    "                    milestmu = int(milestmu) * 1000\n",
    "            elif \"K\" in str(milestmu):\n",
    "                index = mileagedesc.index(\"ilometers\")\n",
    "                tempmile = mileagedesc[0:index].partition(\" \")[0]\n",
    "                if \"k\" in tempmile or \"K\" in tempmile:\n",
    "                    miles = re.sub(r\"[^0-9]\", \"\", tempmile)\n",
    "                    miles = int(miles) * 1000\n",
    "        except:\n",
    "            print('failed')\n",
    "            \n",
    "        #Check for bespoke bodies\n",
    "        if 'Bertone' in title:\n",
    "            bertone = 1\n",
    "        else:\n",
    "            bertone = 0\n",
    "        if 'Zagato' in title:\n",
    "            zagato = 1\n",
    "        else:\n",
    "            zagato = 0\n",
    "\n",
    "        Car = [lotnumb, desc, trans, year, miles, \\\n",
    "            milestmu, engine, turbo, super, numbmatch, \\\n",
    "            hardtop, overdrive, scratch, paintbub, metalrepair, \\\n",
    "            replica, rust, refurbished, restored, project, sold, \\\n",
    "            month, yearsold, price, town, state, zagato, bertone, \\\n",
    "            chassis, specialdesc, mileagedesc, enginedesc, transdesc, \\\n",
    "            paintdesc, interiordesc, carbdesc, wheeldesc, brakedesc, suspdesc, listings[i], make, model]\n",
    "        \n",
    "        carlist.append(Car)\n",
    "    \n",
    "    else:\n",
    "        print('Discarded')\n",
    "\n",
    "# Export to DataFrame\n",
    "df = pd.DataFrame(carlist, columns=['Lot Number', 'Description', 'Transmission', 'Model Year', 'Mileage', 'Mileage (if TMU)',\\\n",
    "    'Engine', 'Turbocharged', 'Supercharged', 'Numbers Matching', 'Hardtop', 'Overdrive', 'Scratch', 'Paint Bubble', 'Metal Repair', \\\n",
    "    'Replica', 'Rust','Refurbished','Restored', 'Project', 'Sold', 'Month', 'Year', 'Price', 'City', 'State', 'Zagato', \\\n",
    "    'Bertone', 'Chassis', 'Special Description', 'Mileage Description', 'Engine Description', 'Transmission Description',\\\n",
    "    'Paint Description', 'Interior Description','Carburetor Description', 'Wheels Description', 'Brakes Description', 'Suspension Description', 'URL', 'Make', 'Model'])\n",
    "\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data as Excel File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/' + make + '/'\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    print('Path already exists')\n",
    "\n",
    "df.to_excel(path + make + '_' + model + '.xlsx')\n",
    "\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_steph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf3fa1aa09a0c6fa190c404f31caacef32adaf1e8c659fe741a42dc3ade0d5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
